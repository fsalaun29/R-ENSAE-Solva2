---
title: "Projet de Langage R en Actuariat"
author: "Marie GANON, Daniel NKAMENI, Florian SALAUN"
output: 
  pdf_document:
    fig_width: 5
    fig_height: 3
    number_sections: true
header-includes:
  - \usepackage{subfig}
editor_options: 
  markdown: 
    wrap: 72
---

```{=tex}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\tq}{\: | \:}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\diff}{\: d}
\newcommand{\1}{\mathbf{1}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\Ber}{\mathrm{Ber}}
\newcommand{\Poiss}{\mathrm{Poiss}}
\newcommand{\Exp}{\mathrm{Exp}}
\newcommand{\Nor}{\mathcal{N}}
\newcommand{\esp}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\Biais}{\mathrm{Biais}}
\newcommand{\EQM}{\mathrm{EQM}}
\newcommand{\Limf}[2]{\underset{#1 \rightarrow #2}{\longrightarrow}}
\newcommand{\cps}[2]{\underset{#1 \to #2}{\overset{p.s.}{\longrightarrow}}}
\newcommand{\cloi}[2]{\underset{#1 \to #2}{\overset{(d)}{\longrightarrow}}}
\newcommand{\SCR}{\mathrm{SCR}}
\newcommand{\arctanh}{\mathrm{arctanh}}
```

------------------------------------------------------------------------

```{r setup, include=FALSE}
# Exécuter cette ligne pour tester le script LaTeX : 
# knitr::opts_chunk$set(eval = FALSE)
knitr::opts_chunk$set(echo = F, fig.pos= "h", warning=FALSE)
library(knitr)
library(copula)
library(matrixStats)
library(spatstat)
library(papeR)
library(cowplot)
library(ggplot2)
library(microbenchmark)
options(xtable.comment = FALSE)
set.seed(0)
```

# Partie 1 - Agrégation simple des risques

Avant toute chose, nous stockons les valeurs des paramètres des lois
normales et des copules.

```{r}
mu_log = 16.97
sigma_log = 0.08398
rho_C = 0.25
alpha_C = 0.35

n = 10^7

margin_laws_S = c("lnorm", "lnorm")
margin_params_S = list(list(meanlog=mu_log, sdlog=sigma_log), 
                       list(meanlog=mu_log, sdlog=sigma_log))
```

Puis nous créons deux fonctions afin de calculer le *Best Estimate* (BE)
et le *Solvency Capital Requirement* (SCR) d'une variable aléatoire $X$.
Ces deux grandeurs sont définies de la manière suivante:
$$BE(X) = \mathbb{E}(X) \text{ et } SCR(X) = VaR_{99,5\%}(X)-BE(X)$$ où
$VaR_{99,5\%}(X)$ est la *Value at Risk* au niveau 99,5% correspond au
quantile de niveau 99,5%.

```{r}
BE <- function(S){
  return(mean(S))
}

SCR <- function(S){
  return(quantile(S, 0.995)[[1]]-BE(S))
}

SCR_mat <- function(S){
  return(colQuantiles(S, prob=0.995)-colMeans(S))
}
```

## Modélisation avec copule gaussienne

### Formule standard

On définit la copule gaussienne à partir des paramètres introduits plus
haut, et on génère $n = 10^7$ simulations de $(S_1, S_2)$.

```{r}
cop_norm = normalCopula(rho_C, dim=2)
myMvd_cop_norm = mvdc(copula=cop_norm, margins=margin_laws_S,
                   paramMargins=margin_params_S)
S_i_norm = rMvdc(n, myMvd_cop_norm)
```

Afin d'obtenir un intervalle de confiance de $\mathrm{SCR}(S_1+S_2)$, il
nous faut tout d'abord calculer les SCR des deux risques individuels. On
peut le faire de manière computationnellement efficace en utilisant les
fonctions \texttt{colMeans} (pour le calcul du \textit{Best Estimate})
et \texttt{colQuantiles} (nécessitant l'importation de la librairie
\texttt{MatrixStats}).

```{r, results="asis"}
SCR_i_norm = SCR_mat(S_i_norm)
cat(paste0("[", round(SCR_i_norm[1]), ", ", round(SCR_i_norm[2]), "]"))
```

La difficulté consiste dans l'estimation du coefficient de corrélation
entre $S_1$ et $S_2$. L'énoncé nous rappelle que :
$$ \sqrt{n} (\widehat{r}_n - r) \underset{n \to +\infty}{\overset{(d)}{\longrightarrow}} \mathcal{N}(0,1), \quad \widehat{r}_n = \frac{1}{2} \ln \left( \frac{1+\widehat{\rho}_n}{1-\widehat{\rho}_n} \right) = \mathrm{arctanh}(\widehat{\rho}_n), \quad r = \mathrm{arctanh}(\rho). $$

Comme la fonction $x \mapsto \tanh(x)$ est dérivable sur $\mathbb{R}$,
d'après le théorème de la méthode Delta :

```{=tex}
\begin{align*}
  \sqrt{n}(\widehat{\rho}_n - \rho) &= \sqrt{n} (\tanh(\widehat{r}_n) - \tanh(r)) \\
  &\underset{n \to +\infty}{\overset{(d)}{\longrightarrow}} \mathcal{N}(0, (\tanh'(r))^2)
\end{align*}
```
avec $\tanh'(r) = 1 - \tanh(r)^2 = 1 - \rho^2$. Ainsi :
$$ \sqrt{n} \frac{\widehat{\rho}_n - \rho}{1-\rho^2} \underset{n \to +\infty}{\overset{(d)}{\longrightarrow}} Y $$
avec $Y \sim \mathcal{N}(0,1)$. Dans cette formule, deux termes
dépendent du coefficient inconnu $\rho$, ce qui ne nous permet pas de
déduire directement des intervalles de confiance. Or, on sait que
$\widehat{\rho}_n \underset{n \to +\infty}{\overset{p.s.}{\longrightarrow}} \rho$
car
$$ \widehat{\mathrm{Cov}}(X,Y) \underset{n \to +\infty}{\overset{p.s.}{\longrightarrow}} \mathrm{Cov}(X,Y), \quad \widehat{\mathrm{Var}}(X) \underset{n \to +\infty}{\overset{p.s.}{\longrightarrow}} \mathrm{Var}(X), \quad \widehat{\mathrm{Var}}(Y) \underset{n \to +\infty}{\overset{p.s.}{\longrightarrow}} \mathrm{Var}(Y) $$
et car la fonction $(x,y,z) \mapsto \frac{x}{\sqrt{y} \sqrt{z}}$ est
continue sur $\mathbb{R}\times (\mathbb{R}_+^*)^2$. Par le lemme de
Slutsky, on en déduit que :
$$ \left( \sqrt{n} \frac{\hat{\rho}_n - \rho}{1-\rho^2}, \hat{\rho}_n \right) \underset{n \to +\infty}{\overset{(d)}{\longrightarrow}} (Y, \rho). $$
Or, la fonction $(u,v) \mapsto u \frac{1-\rho^2}{1-v^2}$ est continue
sur $\mathbb{R}\times ]-1,1[$, d'où :
$$ \sqrt{n} \frac{\hat{\rho}_n - \rho}{1-\hat{\rho}_n^2} \underset{n \to +\infty}{\overset{(d)}{\longrightarrow}} Y. $$

Pour que ce résultat soit rigoureusement valide, il faudrait s'assurer
que $\mathbb{P}(\hat{\rho}_n = 1) = 0$. Sinon, il faudrait rajouter un
terme $\varepsilon > 0$ arbitrairement petit devant le terme
$1-\hat{\rho}_n^2$, ce qui aurait une influence arbitrairement petite
sur l'intervalle de confiance obtenu. On décide donc d'omettre cette
vérification.

Soit $y \in \mathbb{R}$. D'après la convergence en loi, pour $n$ assez
grand ($10^7$ convient largement), on a :
$$ \mathbb{P}\left( \sqrt{n} \frac{|\hat{\rho}_n - \rho|}{1-\hat{\rho}_n^2} \leq y \right) \approx \mathbb{P}(|Y| \leq y) $$
soit
$$ \mathbb{P}\left( |\hat{\rho}_n - \rho| \leq y \frac{1-\hat{\rho}_n^2}{\sqrt{n}} \right) \approx \mathbb{P}(|Y| \leq y). $$

Or, $\mathbb{P}(|Y| \leq y) = 2 \mathbb{P}(Y \leq y) - 1$ par symétrie
de $Y$. De plus, $2 \mathbb{P}(Y \leq y) - 1 = 0.995$ revient à
$\mathbb{P}(Y \leq y) = 0.9975$, et on peut en déduire $y = q_{0.9975}$
à partir de la fonction \texttt{qnorm} de R :

```{r, results="asis"}
q <- qnorm(0.9975, 0, 1)
cat(q)
```

Finalement, avec probabilité 99,5% :
$$ \rho \in \left[ \hat{\rho}_n - q_{0.9975} \frac{1-\hat{\rho}_n^2}{\sqrt{n}}, \hat{\rho}_n + q_{0.9975} \frac{1-\hat{\rho}_n^2}{\sqrt{n}} \right] $$

et on peut déduire l'application numérique :

```{r, results="asis"}
r1 = cor(S_i_norm)[1,2]
r2 = r1 + c(-q * (1-r1^2)/sqrt(n), q * (1-r1^2)/sqrt(n))
cat(paste0("[", r2[1], ", ", r2[2], "]"))
#r2
```

ainsi qu'un intervalle de confiance de $\mathrm{SCR}(S_1+S_2)$ à 99.5%
via la formule
$$ \mathrm{SCR}(S_1+S_2) = \sqrt{\mathrm{SCR}(S_1)^2 + \mathrm{SCR}(S_2)^2 + 2 \rho(S_1, S_2) \mathrm{SCR}(S_1) \mathrm{SCR}(S_2)}. $$

```{r, results="asis"}
SCR_FS <- function(SCR_i, rho){
  sqrt(sum(SCR_i^2) + 2*rho*prod(SCR_i))
}

SCR_FS_norm <- SCR_FS(SCR_i_norm, r1)
cat(paste0("SCR : ", SCR_FS_norm, "\n"))
```

```{r, results="asis"}
IC_FS_norm <- SCR_FS(SCR_i_norm, r2)
cat(paste0("Intervalle de confiance : [", round(IC_FS_norm[1]), ", ", round(IC_FS_norm[2]), "]"))
```

### Approche exacte

Dans cette partie, nous calculons le $\mathrm{SCR}$ de $S = S_1+S_2$ en
utilisant une approche directe. Il sera question de simuler $S_1$ et
$S_2$, de calculer $S$ et ensuite d'appliquer la formule:
$$\mathrm{SCR}(S)=VaR_{99,5\%}(S)-BE(S) \text{ où }BE(S) = \mathbb{E}(S) $$
Le *Best Estimate* (moyenne) de $S \text{, }BE(S)$ est égal à:

```{r, results="asis"}
S_norm = S_i_norm[,1]+S_i_norm[,2]
cat(round(BE(S_norm)))
```

La $VaR_{99,5\%}(S)$ est obtenue en calculant le quantile d'ordre 99,5%
de $S$. Elle est égale à:

```{r, results="asis"}
VaR_S_norm = quantile(S_norm, probs = 0.995)[[1]]
cat(round(VaR_S_norm))
```

Le $\mathrm{SCR}(S)$ est donc égal à :

```{r, results="asis"}
SCR_norm = VaR_S_norm - BE(S_norm)
cat(round(SCR_norm))
```

Calculons à présent l'intervalle de confiance à $99,5\%$ de ce SCR. Nous
savons que :
$$\sqrt{n} (\hat{q}_n^\alpha - q_\alpha)  \underset{n \to +\infty}{\overset{(d)}{\longrightarrow}} \mathcal{N}\left(0,\frac{\alpha(1-\alpha)}{f_S(q_\alpha)}\right)$$
où $f_S(q_\alpha)$ est la densité de $S$ au point $q_\alpha$. On peut
réécrire cette convergence en loi sous la forme:
$$\sqrt{\frac{nf_S(q_\alpha)}{\alpha(1-\alpha)}} (\hat{q}_n^\alpha - q_\alpha)  \underset{n \to +\infty}{\overset{(d)}{\longrightarrow}} Y \text{ où } Y\sim\mathcal{N}(0,1)$$
Comme à la question précédente, deux termes dépendent du coefficient
inconnu $q_\alpha$. Pour déterminer l'intervalle de confiance, on fera
l'hypothèse que
$\hat{q}_n \underset{n \to +\infty}{\overset{p.s.}{\longrightarrow}} q_\alpha$.
Puisque la fonction $f_S$ est continue sur $\mathbb{R}$, on a
$f_S(\hat{q}_n) \underset{n \to +\infty}{\overset{p.s.}{\longrightarrow}} f_S(q_\alpha)$
et par le Lemme de Slutsky, on en déduit que:

$$\left(\sqrt{\frac{nf_S(q_\alpha)}{\alpha(1-\alpha)}} (\hat{q}_n^\alpha - q_\alpha),f_S(\hat{q}_n^\alpha) \right)  \underset{n \to +\infty}{\overset{(d)}{\longrightarrow}} (Y,f_S(q_\alpha))$$
Or, la fonction $(u,v) \mapsto u \frac{\sqrt{v}}{\sqrt{f_S(q_\alpha)}}$
est continue sur $\mathbb{R}\times \mathbb{R}$, d'où :
$$\sqrt{\frac{nf_S(\hat{q}_\alpha)}{\alpha(1-\alpha)}} (\hat{q}_n^\alpha - q_\alpha)  \underset{n \to +\infty}{\overset{(d)}{\longrightarrow}} Y$$
Pour la suite, il nous faut $f_S(\hat{q}_\alpha)$. Nous allons estimer
la densité de $S$, $f_S$ par la méthode des noyaux. Il s'agit d'une
méthode non paramétrique permettant d'estimer la densité de probabilité
d'une variable aléatoire continue. Cette estimation est donnée par:
$$\hat{f}(x)=\frac{1}{nh}\sum_{i=1}^{n}K\left[\frac{x-x_i}{h}\right]$$
Où $h$ est le pas, $n$ la taille de l'échantillon et $K$ est le noyau
choisi. Ce noyau est en général gaussien, uniforme ou triangulaire. La
fonction \texttt{density} de R permet de faire cette estimation sans
difficulté. L'estimation de la densité de $S$ est représentée sur la Figure 1.

```{r fig.align="center", fig.cap =  "Densité de S - Copule gaussienne"}
density_S_norm = density(S_norm, n = 1024)
ggplot(data.frame("S" = S_norm), aes(x=S)) + 
  theme_bw()+
  xlab("Valeur de S") + ylab("Densité")+
  geom_density()
```

L'estimation de cette densité au point
$\hat{q}_{0,995} = \widehat{VaR}_{99,5\%}$ est obtenue par interpolation
grâce à la fonction \texttt{approx} de R et est égale à :

```{r, results="asis"}
f_density_S_norm = approx(density_S_norm$x, density_S_norm$y, xout = VaR_S_norm)$y
cat(f_density_S_norm)
```

Soit $y \in \mathbb{R}$. Pour $n$ assez grand, on a :
$$ \mathbb{P}\left( \sqrt{\frac{nf_S(\hat{q}_\alpha)}{\alpha(1-\alpha)}} |(\hat{q}_n^\alpha - q_\alpha)| \leq y \right) \approx \mathbb{P}(|Y| \leq y) $$
soit
$$ \mathbb{P}\left( |(\hat{q}_n^\alpha - q_\alpha)| \leq y \sqrt{\frac{\alpha(1-\alpha)}{nf_S(\hat{q}_\alpha)}} \right) \approx \mathbb{P}(|Y| \leq y). $$

Or, $\mathbb{P}(|Y| \leq y) = 2 \mathbb{P}(Y \leq y) - 1$ par symétrie
de $Y$. De plus, $2 \mathbb{P}(Y \leq y) - 1 = 0,995$ revient à
$\mathbb{P}(Y \leq y) = 0.9975$, et on peut en déduire
$y = q_{0,9975}^{norm}$ à partir de la fonction \texttt{qnorm} de R :

```{r, results="asis"}
q <- qnorm(0.9975, 0, 1)
cat(q)
```

Finalement, avec probabilité 99,5% :
$$ q_{0,995} = VaR_{99,5\%} \in \left[ \widehat{VaR}_{99,5\%} - q_{0,9975}^{norm}\sqrt{\frac{\alpha(1-\alpha)}{nf_S(\hat{q}_\alpha)}}, \widehat{VaR}_{99,5\%} + q_{0,9975}^{norm}\sqrt{\frac{\alpha(1-\alpha)}{nf_S(\hat{q}_\alpha)}} \right] $$
Où $q_{0.9975}^{norm}$ est le quantile d'ordre $0,9975$ de la loi
normale centrée reduite. On peut déduire l'intervalle de confiance à
$99,5\%$ de $VaR_{99,5\%}$ :

```{r, results="asis"}
A = sqrt((0.995*(1-0.995))/f_density_S_norm)
CI_min = VaR_S_norm - qnorm(0.9975)*(A/sqrt(n))
CI_max = VaR_S_norm + qnorm(0.9975)*(A/sqrt(n))  
CI = c(CI_min,CI_max)
cat(paste0("[", round(CI[1]), ", ", round(CI[2]), "]"))
```

ainsi qu'un intervalle de confiance de $\mathrm{SCR}(S_1+S_2)$ à 99.5%
via la formule $$ \mathrm{SCR}(S)=VaR_{99,5\%}(S)-BE(S) $$

```{r, results="asis"}
IC_norm <- CI - BE(S_norm)
cat(paste0("[", round(IC_norm[1]), ", ", round(IC_norm[2]), "]"))

```

## Modélisation avec copule de Clayton

On définit la copule de Clayton à partir des paramètres introduits plus
haut, et on génère $n = 10^7$ simulations de $(S_1, S_2)$.

```{r}
cop_clayton = rotCopula(claytonCopula(alpha_C, dim=2))
myMvd_cop_clayton = mvdc(copula=cop_clayton, margins=margin_laws_S,
              paramMargins=margin_params_S)
S_i_clayton = rMvdc(n, myMvd_cop_clayton)
```

### Formule standard

On répète les étapes effectuées avec une modélisation par copule
gaussienne. On calcule d'abord les SCR individuels :

```{r, results="asis"}
SCR_i_clayton = SCR_mat(S_i_clayton)
cat(round(SCR_i_clayton))
```

On détermine ensuite un intervalle de confiance à 99.5% du coefficient
de corrélation de la même manière que précédemment :

```{r, results="asis"}
r1 = cor(S_i_clayton)[1,2]
r2 = r1 + c(-q * (1-r1^2)/sqrt(n), q * (1-r1^2)/sqrt(n))
cat(paste0("[", r2[1], ", ", r2[2], "]"))

```

Enfin, on peut en déduire un intervalle de confiance à 99.5% de
$\mathrm{SCR}(S_1+S_2)$ :

```{r, results="asis"}
SCR_FS_clayton <- SCR_FS(SCR_i_clayton, r1)
cat(paste0("SCR : ", SCR_FS_clayton, "\n"))
```

```{r, results="asis"}
IC_FS_clayton <- SCR_FS(SCR_i_clayton, r2)
cat(paste0("Intervalle de confiance : [", round(IC_FS_clayton[1]), ", ", round(IC_FS_clayton[2]), "]"))
```

### Approche exacte

La démarche à suivre dans cette partie est identique à celle utilisée
avec la copule gaussienne. En simulant $S_1$ et $S_2$ grâce à la copule
de Clayton, le *Best Estimate* de $S \text{, }BE(S)$ est égal à:

```{r, results="asis"}
S_clayton <- S_i_clayton[,1]+S_i_clayton[,2]
cat(BE(S_clayton))
```

La $VaR_{99,5\%}(S)$ est égale à:

```{r, results="asis"}
VaR_S_clayton <- quantile(S_clayton, probs = 0.995)[[1]]
cat(VaR_S_clayton)
```

Le $\mathrm{SCR}(S)$ est donc égal à :

```{r, results="asis"}
SCR_clayton <- SCR(S_clayton)
cat(SCR_clayton)
```

L'estimation de la densité de $S$ est représentée sur la Figure 2.

```{r, fig.align="center", fig.cap = "Densité de S - Copule de Clayton"}
density_S_clayton = density(S_clayton, n = 1024)
ggplot(data.frame("S" = S_clayton), aes(x=S)) + 
  theme_bw()+
  xlab("Valeur de S") + ylab("Densité")+
  geom_density()
```

L'estimation de cette densité au point
$\hat{q}_{0,995} = \widehat{VaR}_{99,5\%}$ est égale à :

```{r}
f_density_S_clayton = approx(density_S_clayton$x, density_S_clayton$y, xout = VaR_S_clayton)$y
```

L'intervalle de confiance à $99,5\%$ de $VaR_{99,5\%}$ est :

```{r, results="asis"}
A = sqrt((0.995*(1-0.995))/f_density_S_clayton)
CI_min = VaR_S_clayton - qnorm(0.95)*(A/sqrt(n))
CI_max = VaR_S_clayton + qnorm(0.95)*(A/sqrt(n))  
CI = c(CI_min,CI_max)
cat(paste0("[", round(CI[1]), ", ", round(CI[2]), "]"))

```

Et l'intervalle de confiance à $99,5\%$ du $\mathrm{SCR}(S_1+S_2)$ est :

```{r, results="asis"}
IC_clayton <- CI - mean(S_clayton)
cat(paste0("[", round(IC_clayton[1]), ", ", round(IC_clayton[2]), "]"))

```

## Comparaison des méthodes

Afin de comparer les deux méthodes, nous présentons dans la Table
1 l'ensemble de nos résultats de SCR.

```{r, results="asis"}
df_comp <- data.frame("Formule standard" = c(SCR_FS_norm, SCR_FS_clayton), "Approche exacte" = c(SCR_norm, SCR_clayton), row.names = c("Copule gaussienne", "Copule de Clayton"))
#xtable(df_comp, caption = "Récapitulatif des SCR calculés")
```

```{r table1}
  knitr::kable(df_comp, caption = "Récapitulatif des SCR calculés")
```

Lorsque l'on applique la formule standard pour calculer le $\mathrm{SCR}$, le choix de la copule n'a pas énormémement d'importance. Toutefois, ce dernier devient déterminant dans le cadre de l'approche exacte. En effet, choisir une copule de Clayton pour modéliser la dépendance entre les deux risques conduit à surestimer fortement le $\mathrm{SCR}$, par rapport au calcul effectué en formule standard. A l'inverse, la copule gaussienne diminue légèrement l'estimation du $\mathrm{SCR}$ par rapport au calcul effectué en formule standard.

Nous pouvons également comparer les intervalles de confiance pour chacune des approches et chaque structure de copule utilisée. Dans les tables suivantes, nous appelerons donc min(SCR) et max(SCR) les limites des intervalles de confiance à 99,5%.
```{r, results="asis"}
df_comp_norm <- data.frame("Formule standard" = c(IC_FS_norm[1], SCR_FS_norm, IC_FS_norm[2]), "Approche exacte" = c(IC_norm[1], SCR_norm, IC_norm[2]), row.names = c("min(SCR)", "SCR", "max(SCR)"))
df_comp_clayton <- data.frame("Formule standard" = c(IC_FS_clayton[1], SCR_FS_clayton, IC_FS_clayton[2]), "Approche exacte" = c(IC_clayton[1], SCR_clayton, IC_clayton[2]), row.names = c("min(SCR)", "SCR", "max(SCR)"))

xtable(df_comp_norm, caption = "Récapitulatif des SCR calculés pour la copule gaussienne")
xtable(df_comp_clayton, caption = "Récapitulatif des SCR calculés pour la copule de Clayton")
```

# Partie 2 - Agrégation des risques par somme aléatoire

Nous commençons par définir tous les paramètres qui nous seront utiles
par la suite.

```{r}
n1 <- 8477
p1 <- 0.3679
n2 <- 8
p2 <- 0.2191
k <- 50000
s <- 67117
xi <- 0.4270
n <- 10^5

margin_laws_N = c("nbinom", "nbinom")
margin_params_N = list(list(size=n1, prob=p1), 
                       list(size=n2, prob=p2))

cop_norm = normalCopula(rho_C, dim=2)
Mvd_cop_norm = mvdc(copula=cop_norm, margins=margin_laws_N,
                   paramMargins=margin_params_N)

cop_clayton <- rotCopula(claytonCopula(alpha_C, dim=2))
Mvd_cop_clayton = mvdc(copula=cop_clayton, margins=margin_laws_N,
                   paramMargins=margin_params_N)
```

Pour simuler $S_1$ et $S_2$ nous procédons en plusieurs étapes. Nous
simulons dans un premier temps les deux nombres de sinistres, avec des
lois marginales négatives binomiales et la structure de copule choisie
(gaussienne ou Clayton). Puis, pour chaque simulation :

1.  Nous générons les $X_n^1$ selon la loi
    $\mathcal{LN}(\mu_{log}, \sigma_{log})$.
2.  Nous générons les $U_n$ selon la loi $\mathcal{U}([0,1])$.
3.  Nous calculons $X_n^2 = k + \frac{s}{\xi}(U_n^{-\xi}-1)$.
4.  Enfin nous sommons respectivement les $X_n^1$ et les $X_n^2$ pour
    obtenir $S_1$ et $S_2$.

Plusieurs approches ont été tentées afin d'optimiser le temps
d'exécution ainsi que l'usage de mémoire vive. Une telle démarche
s'avère nécessaire au vu du grand nombre de simulations à générer. Tout
d'abord, nous avons testé une fonction R sans boucle utilisant
l'instruction \texttt{sapply}. Ensuite, nous avons utilisé un code C
(inspiré du cours) contenant une boucle afin d'accélérer l'exécution. Nous avons lancé un \emph{microbenchmark} permettant de répéter 100 fois
la génération de $n_0 = 10^2$ simulations de $S_1$ à partir de ces deux
méthodes. Il en résulte la Figure 3.

```{r, include = FALSE, echo=FALSE, message=FALSE}
writeLines('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")
system("R CMD SHLIB C/rsum.c")
dyn.load("C/rsum.dll")
#system("R CMD SHLIB C/rsum2.c")
#dyn.load("C/rsum2.dll")

rsum <- function(N1, mu, sigma){
  n = length(N1)
  return(.C("rsum",
            N1 = as.double(N1),
            n = as.integer(n),
            mu = as.double(mu),
            sigma = as.double(sigma),
            S = numeric(n)
  )$S)
}

rsum2 <- function(N2, k, s, xi){
  n = length(N2)
  return(.C("rsum2",
            N2 = as.double(N2),
            n = as.integer(n),
            k = as.double(k),
            s = as.double(s),
            xi = as.double(xi),
            S = numeric(n)
  )$S)
}

simu_somme_aleatoire <- function(n,mvd){
  nb_sinistres <- rMvdc(n,mvd)
  #nb_S1 <- nb_sinistres[,1]
  #nb_S2 <- nb_sinistres[,2]
  
  S1 <- rsum(nb_sinistres[,1], mu_log, sigma_log)
  S2 <- rsum2(nb_sinistres[,2], k, s, xi)

  
  #S1 <- sapply(1:n, function(i){
  #  sum(rlnorm(nb_S1[i], meanlog = mu_log, sdlog = sigma_log))
  #})
  
  #S2 <- sapply(1:n, function(i){
  #  U <- runif(nb_S2[i])
  #  sum(k+s/xi*(U^(-xi)-1))
  #})
  return(matrix(c(S1,S2), nrow = n, ncol= 2, byrow = F))
}
```


```{r}

  nb_S1 <- rMvdc(10^2, Mvd_cop_norm)[,1]
  #nb_S1 <- nb_sinistres[,1]
  #nb_S1 <- simu_somme_aleatoire(10^2, Mvd_cop_norm)[,1]
  
  mb1 <- microbenchmark(
    S1 <- sapply(nb_S1, function(x)  sum(rlnorm(x, meanlog = mu_log, sdlog = sigma_log)))
  )
  
  mb2 <- microbenchmark(
    S1 <- rsum(nb_S1, mu_log, sigma_log)
  )
```

```{r, echo=FALSE, fig.align = 'center', fig.cap="Résultats des microbenchmarks en (A) avec un apply sur R et en (B) avec du code C", fig.subcap=c('Exécution sur R', 'Exécution sur C'), warning=FALSE, fig.width=6, fig.height=4}

  df_mb1 <- data.frame("time" = mb1$time/1000000)
  df_mb2 <- data.frame("time" = mb2$time/1000000)
  min_x <- min(c(mb1$time/1000000, mb2$time/1000000))
  max_x <- max(c(mb1$time/1000000, mb2$time/1000000))
  
  p1 <- ggplot(df_mb1, aes(x=time)) + 
        theme_bw() + 
        labs(y="Nombre d'exécutions", x="Temps d'exécution (ms)") +
        xlim(min_x, max_x) + 
        geom_histogram(binwidth=20, color="white", fill="lightgrey")+
        geom_vline(xintercept=median(mb1$time/1000000), color = 'darkred')+
        geom_label(x=median(mb1$time/1000000), y=2, label="Médiane", label.size = 0.25, colour = 'darkred')

  
  p2 <- ggplot(df_mb2, aes(x=time)) +
        theme_bw() + 
        labs(y="Nombre d'exécutions", x="Temps d'exécution (ms)") +  
        xlim(min_x, max_x) + 
        geom_histogram(binwidth=20, color="white", fill="lightgrey")+
        geom_vline(xintercept=median(mb2$time/1000000), color = 'darkred')+
        geom_label(x=median(mb2$time/1000000), y=2, label="Médiane", label.size = 0.25, colour = 'darkred')

  
  plot_grid(p1, p2, labels = c('A', 'B'))

```
Nous avons constaté, sur une machine équipée d'un processeur Intel Core i7-10510U (4 coeurs, 1.8 GhZ) et d'une mémoire vive de 8 Go, que le code C permettait de réduire le temps d'exécution d'environ un quart par rapport à un code R bien optimisé. En outre, cette solution est optimale en termes d'utilisation de mémoire vive. Par comparaison, la proposition qui consistait à construire sur R une matrice de taille $n \times \max(N)$ n'a pas pu aboutir sur nos machines pour $n = 10^5$, car elle conduisait à libérer plus de 10 Go de mémoire vive.


\newpage
## Modélisation avec copule gaussienne

Dans cette partie, nous appliquons une structure de copule gaussienne de
paramètre $\rho_C$.

```{r}
S_i_norm <- simu_somme_aleatoire(n,Mvd_cop_norm)
```

### Formule standard

Pour appliquer la formule standard, nous avons tout d'abord besoin de
calculer le coefficient de corrélation linéaire $\rho$ entre $S_1$ et
$S_2$. Celui-ci se calcule empiriquement de la manière suivante:
$$\widehat{\rho}_n = \frac{\sum_{i = 1}^n(S_1^i- \overline{S_1})(S_2^i- \overline{S_2})}{\sqrt{\sum_{i = 1}^n(S_1^i- \overline{S_1})^2}\sqrt{\sum_{i = 1}^n(S_2^i- \overline{S_2})^2}}$$

La formule standard donne le SCR suivant:

```{r, results="asis"}
SCR_FS_norm <- SCR_FS(SCR_mat(S_i_norm), cor(S_i_norm)[1,2])
cat(round(SCR_FS_norm))
```

### Modèle agrégé

En calculant le SCR de manière agrégée, en posant $S = S_1+S_2$, nous
obtenons:

```{r, results="asis"}
S_norm <- S_i_norm[,1]+S_i_norm[,2]
SCR_norm <- SCR(S_norm)
cat(round(SCR_norm))
```

Nous cherchons également à déterminer un intervalle de confiance pour le
SCR. Pour ce faire, nous reprenons les calculs effectués dans la partie
1 pour l'approche exacte. Nous estimons donc la Value at Risk, ou quantile d'ordre $\alpha$=99,5\% via le théorème central limite suivant:

$$\sqrt{\frac{nf_S(q_\alpha)}{\alpha(1-\alpha)}} (\widehat{q}_n^\alpha - q_\alpha)  \underset{n \to +\infty}{\overset{(d)}{\longrightarrow}} Y \text{ où } Y\sim\mathcal{N}(0,1)$$

Nous devons donc approximer la densité de $S$. Par la méthode des
noyaux, nous obtenons la fonction tracée sur la Figure 4.

```{r fig.align="center", fig.cap=  "Densité de S - Copule gaussienne"}
density_S_norm = density(S_norm, n = 1024)
ggplot(data.frame("S" = S_norm), aes(x=S)) + 
  theme_bw()+
  xlab("Valeur de S") + ylab("Densité")+
  geom_density()
```

On doit ensuite approximer cette fonction en la valeur du quantile
d'ordre 99,5%.

```{r}
VaR_S_norm <- quantile(S_norm, probs = 0.995)[[1]]
fS_q_995 <- approx(density_S_norm$x, density_S_norm$y, xout = VaR_S_norm)$y
```

Puis nous calculons l'intervalle de confiance sur le quantile d'ordre
99,5%.

```{r, results="asis"}
A = sqrt((0.995*(1-0.995))/fS_q_995)
CI_min = VaR_S_norm - qnorm(0.9975)*(A/sqrt(n))
CI_max = VaR_S_norm + qnorm(0.9975)*(A/sqrt(n))  
CI = c(CI_min,CI_max)
cat(paste0("[", round(CI[1]), ", ", round(CI[2]), "]"))

```

Une fois l'intervalle de confiance calculé pour le quantile d'ordre
99,5% de $S$, soit $VaR_{99,5\%}$, nous pouvons alors retrancher le Best
estimate pour obtenir un intervalle de confiance du SCR au niveau 99,5%.

```{r, results="asis"}
IC_norm <- CI-BE(S_norm)
cat(paste0("[", round(IC_norm[1]), ", ", round(IC_norm[2]), "]"))

```

## Modélisation avec copule de Clayton

Dans cette partie, nous appliquons une structure de copule de Clayton
inversée de paramètre $\alpha_C$.

```{r}
S_i_clayton <- simu_somme_aleatoire(n, Mvd_cop_clayton)
```

### Formule standard

La formule standard donne le SCR suivant:

```{r, results="asis"}
SCR_FS_clayton <- SCR_FS(SCR_i_clayton, cor(S_i_clayton)[1,2])
cat(round(SCR_FS_clayton))
```

### Modèle agrégé

En calculant le SCR de manière agrégée, en posant $S = S_1+S_2$, nous
obtenons:

```{r, results="asis"}
S_clayton <- S_i_clayton[,1]+S_i_clayton[,2]
SCR_clayton <- SCR(S_clayton)
cat(round(SCR_clayton))
```

Nous cherchons également à déterminer un intervalle de confiance pour le
SCR. Comme précédemment, nous approximons la densité de $S$ par la
méthode des noyaux, puis nous la traçons sur la Figure 5.

```{r, fig.align="center", fig.cap=  "Densité de S - Copule de Clayton"}
density_S_clayton = density(S_clayton, n = 1024)
ggplot(data.frame("S" = S_clayton), aes(x=S)) + 
  theme_bw()+
  xlab("Valeur de S") + ylab("Densité")+
  geom_density()
```

Nous évaluons ensuite cette fonction en la valeur du quantile d'ordre
99,5%.

```{r}
VaR_S_clayton <- quantile(S_clayton, probs = 0.995)[[1]]
fS_q_995 <- approx(density_S_clayton$x, density_S_clayton$y, xout = VaR_S_clayton)$y
```

Puis nous calculons l'intervalle de confiance sur le quantile d'ordre
99,5%.

```{r, results="asis"}
A = sqrt((0.995*(1-0.995))/fS_q_995)
CI_min = VaR_S_clayton - qnorm(0.9975)*(A/sqrt(n))
CI_max = VaR_S_clayton + qnorm(0.9975)*(A/sqrt(n))  
CI = c(CI_min,CI_max)
cat(paste0("[", round(CI[1]), ", ", round(CI[2]), "]"))

```

Une fois l'intervalle de confiance calculé pour le quantile d'ordre
99,5% de $S$, soit $VaR_{99,5\%}$, nous pouvons alors retrancher le Best
estimate pour obtenir un intervalle de confiance du SCR au niveau 99,5%.

```{r}
IC_clayton <- CI-BE(S_clayton)
```

## Comparaison des méthodes

Afin de comparer les deux méthodes, nous présentons dans la Table
4 l'ensemble de nos résultats de SCR.

```{r, results="asis"}
df_comp <- data.frame("Formule standard" = c(SCR_FS_norm, SCR_FS_clayton), "Modèle agrégé" = c(SCR_norm, SCR_clayton), row.names = c("Copule gaussienne", "Copule de Clayton"))
xtable(df_comp, caption = "Récapitulatif des SCR calculés")
```


*Commentaire à ajouter*

Comparons désormais les intervalles de confiance obtenus via le modèle agrégé. Nous les affichons sur la Table 5 ci-dessous.

```{r, results="asis"}
df_comp_agrege <- data.frame("Copule gaussienne" = c(IC_norm[1], SCR_norm, IC_norm[2]), "Copule de Clayton" = c(IC_clayton[1], SCR_clayton, IC_clayton[2]), row.names = c("min(SCR)", "SCR", "max(SCR)"))
xtable(df_comp_agrege, caption = "Intervalles de confiance obtenus pour le SCR")
```

*Commentaire à ajouter*
